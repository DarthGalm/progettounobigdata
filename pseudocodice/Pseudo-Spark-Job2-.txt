class SparkEsDue {

#support class as Objects

 private static class YearlyTrend implements Serializable {
        public Integer year;
        public Double firstClosePrice;
        public Double lastClosePrice;
        public String firstDate;
        public String lastDate;
        public Double pricesSum;//and mean, later
        public Double volumeSum;//and mean, later
        public Integer count;

        public YearlyTrend(Integer year, Double firstClosePrice, Double lastClosePrice, String firstDate, String lastDate, Double pricesSum, Double volumeSum, Integer count) {
            this.year = year;
            this.firstClosePrice = firstClosePrice;
            this.lastClosePrice = lastClosePrice;
            this.firstDate = firstDate;
            this.lastDate = lastDate;
            this.pricesSum = pricesSum;
            this.volumeSum = volumeSum;
            this.count = count; //come fosse un wordcount, metto 1 e poi sommo
        }
    }

    private static class Trend implements Serializable {
        public Double pricesMean;
        public Double volumeMean;
        public Double percentVarMean;
        public Integer countForPriceAndVolume;
        public Integer countForPercentVar; //come un wordcount metto 1 e poi sommo

        public Trend(Double pricesMean, Double volumeMean, Double percentVarMean, Integer countForPriceAndVolume, Integer countForPercentVar) {
            this.pricesMean = pricesMean;
            this.volumeMean = volumeMean;
            this.percentVarMean = percentVarMean;
            this.countForPriceAndVolume = countForPriceAndVolume;
            this.countForPercentVar = countForPercentVar;
        }
    }

function main() {

create Spark context and conf, takes input files

#maps the input historical stocks to a tuple with only the ticker and the sector
#regex found on StackOverflow at https://stackoverflow.com/questions/34257547/split-string-on-comma-and-ignore-comma-in-double-quotes
JavaRDD<String[]> splitStocks = map(row -> row.split(",(?=([^\"]*\"[^\"]*\")*[^\"]*$)"));
#row[0] and row[3] are first and fourth element of a row in the csv (ticker and sector)
JavaPairRDD<String, String> tickerAndCompanySectorTuple = splitStocks.filterOutRowWith("ticker").mapToPair(row[0],row[3])

#maps the input historical stock prices to a tuple with ticker, and an array with closing price, volume and date
JavaRDD<String[]> splitStockPrices = historicalStockPrices.map(row -> row.split(",(?=([^\"]*\"[^\"]*\")*[^\"]*$)"));
JavaPairRDD<String, String[]> stockPricesTuple = splitStockPrices.filterOutRowWithDateBefore(01/01/2008).mapToPair(row[0], String[][row[2],row[6],row[7]])

#this joins on the key which in the first element of the tuples of the JavaPairRDDs
JavaPairRDD<String, Tuple2<String, String[]>> joinDataset = tickerAndCompanySectorTuple.join(stockPricesTuple);

#groups by ticker, sector and year as key, object Trend as value to store data such as prices and dates to aggregate later
JavaPairRDD<Tuple3<String, String, String>, YearlyTrend> joinDatasetGroupedByTickerSectorYear = joinDataset.mapToPair(
			#ticker sector and year taken by accessing the tuple like this: line._1(), line._2()._1() scala-like
                line -> { new Tuple(ticker, sector, year), new YearlyTrend(#initialized with values taken from records) }

#this is to aggregate all the values sharing the same ticker, sector and year (but different dates of course) into one
JavaPairRDD<Tuple3<String, String, String>, YearlyTrend> reducedDatasetByTickerYearAndSector = joinDatasetGroupedByTickerSectorYear.reduceByKey(
(firstTrend, secondTrend) -> #this aggregates the second in the first, one by one, first is the accumulator, with a lambda function

aggregatedPrice = firstTrend.pricesSum + secondTrend.pricesSum;
aggregatedVolume = firstTrend.volumeSum + secondTrend.volumeSum;
aggregatedCount = firstTrend.count + secondTrend.count;
		//initial date of first trend is later than second's one
		if(firstInitialDate.compareTo(secondInitialDate) > 0) { 
                    chosenFirstDate = secondTrend.firstDate;
                    chosenFirstPrice = secondTrend.firstClosePrice;
                } else {  //date of second trend is later than first's
                    chosenFirstDate = firstTrend.firstDate;
                    chosenFirstPrice = firstTrend.firstClosePrice;
                }
		//final date of first trend is later than second's one
                if(firstFinalDate.compareTo(secondFinalDate) > 0) { 
                    chosenLastDate = firstTrend.lastDate;
                    chosenLastPrice = firstTrend.lastClosePrice;
                } else { //date of second trend is later than first's one
                    chosenLastDate = secondTrend.lastDate;
                    chosenLastPrice = secondTrend.lastClosePrice;
                }

return new YearlyTrend(year, chosenFirstPrice, chosenLastPrice, chosenFirstDate, chosenLastDate, aggregatedPrice, aggregatedVolume, aggregatedCount);
)

#now I have to group by sector and year, aggregating by ticker. for the average, I sum every value but not compute the average now, as explained in:
#https://math.stackexchange.com/questions/115091/is-the-average-of-the-averages-equal-to-the-average-of-all-the-numbers-originall
#I simply return all the values of yearlyTend to use them later on. The Percent Variation is calculated now as it works differently from an average of values.

#first I have to map because the reduceByKey bounds the output to the same input type
JavaPairRDD<Tuple2<String, String>, Trend> newComputationByYearAndSector = reducedDatasetByTickerYearAndSector.mapToPair(line ->
yearlyTrend = line._2() #second element of reducedDatasetByTickerYearAndSector tuples
percentvariation = ((yearlyTrend.lastClosePrice - yearlyTrend.firstClosePrice)/yearlyTrend.firstClosePrice)*100;
#initializes trend with yearlyTrend values and the now calculated percentVariation, count for price and volume average and a 1 for percent variation average)
return (new Tuple(sector,year), new Trend(pricesSum, volumeSum, percentVariation, count, 1)

#aggregates and returns the final sums for sector and year
JavaPairRDD<Tuple2<String, String>, Trend> finalReducedByYearAndSector = newComputationByYearAndSector.reduceByKey(
                (firstTrend, secondTrend) -> {
		Double aggregatedVolumeForMeans = firstTrend.volumeMean + secondTrend.volumeMean;
                Double aggregatedPriceForMeans = firstTrend.pricesMean + secondTrend.pricesMean;
                Double aggregatedPercentVariation = firstTrend.percentVarMean + secondTrend.percentVarMean;
		#a sum of sums
                Integer countTotalForPriceAndVolume = firstTrend.countForPriceAndVolume + secondTrend.countForPriceAndVolume;
		#a sum of ones
                Integer countTotalForPercentVar = firstTrend.countForPercentVar + secondTrend.countForPercentVar; 

                return new Trend(aggregatedPriceForMeans, aggregatedVolumeForMeans, aggregatedPercentVariation, countTotalForPriceAndVolume, countTotalForPercentVar); })

#final calculations for averages and printing to terminal
List<Tuple2<Tuple2<String, String>, Trend>> finalResultAsRDDString = finalReducedByYearAndSector.collect();

Map outputMap = new()

for(line in finalResultAsRDDString) { #this iterates over all the previous results, after collect() transforming an RDD in a List
                    Trend trendToElaborate = line._2();
                    Double finalVolumeMeans = trendToElaborate.volumeMean / trendToElaborate.countForPriceAndVolume;
                    Double finalPricesMeans = trendToElaborate.pricesMean / trendToElaborate.countForPriceAndVolume;
                    Double finalPercentVariation = trendToElaborate.percentVarMean / trendToElaborate.countForPercentVar;

		    String newOutput = #concat of previous values in a readable form

		    String sector = trendToElaborate.sector
		    String year = trendToElaborate.year

		    if(map.contains(sector) { #if map already has a an entry for that sector
			tmpOutput = map.get(sector).append(newOutput) #appends the new result to the old one creating a string, aggregated by sector
			map.replace(sector,tmpOutput) 
			}
		    else{ #map doesn't have an entry for that sector
			map.put(sector, newOutput) 
			}
	
		print finalResultString #formed by printing the newly calculated values, sector and year in the form of
			"Risultati per il settore :" + sector + map.valuesForKey(sector)
}


closeSparkContext()
}

}
